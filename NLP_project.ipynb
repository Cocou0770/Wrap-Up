{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484c67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset as HfDataset\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc197964",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(\"Data/jigsaw-toxic-comment-train.csv\")\n",
    "train_df, val_df = train_test_split(X_df,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fcedad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#GPU ë””ë°”ì´ìŠ¤ ì„¸íŒ…\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00c34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ì ìš© ---\n",
      "ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (Class 0, Class 1 ìˆœì„œ): tensor([0.5529, 5.2258], device='cuda:0')\n",
      "Class 1(toxic)ì— ë” ë†’ì€ í˜ë„í‹°ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMER ëª¨ë¸ \n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "\n",
    "# íƒ€ê²Ÿ ë ˆì´ë¸” ì»¬ëŸ¼ë“¤\n",
    "TARGET_COLUMN = 'toxic'\n",
    "TEXT_COLUMN = \"comment_text\"\n",
    "\n",
    "#í† í°ë‚˜ì´ì €\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#ë¶„ë¥˜ ëª¨ë¸\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,  \n",
    "    problem_type=\"single_label_classification\" \n",
    ")\n",
    "model.to(device) # ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "\n",
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx] # ì´ì œ 2D ë°°ì—´ì´ ì•„ë‹Œ 1D ë°°ì—´ì—ì„œ ê°’ì„ ê°€ì ¸ì˜´\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False, \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', \n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long) \n",
    "        }\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_texts = train_df[TEXT_COLUMN].values\n",
    "\n",
    "# 1D ë°°ì—´ë¡œ ë ˆì´ë¸” ì¶”ì¶œ\n",
    "train_labels = train_df[TARGET_COLUMN].values \n",
    "val_texts = val_df[TEXT_COLUMN].values\n",
    "val_labels = val_df[TARGET_COLUMN].values \n",
    "\n",
    "train_dataset = ToxicCommentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = ToxicCommentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# --- ğŸ‘‡ [ìƒˆ ë‹¨ê³„ ì¶”ê°€] í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ---\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 'balanced' ëª¨ë“œëŠ” ë ˆì´ë¸” ë¹ˆë„ì˜ ì—­ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "# (ì†Œìˆ˜ í´ë˜ìŠ¤ '1'ì´ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤)\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels), # [0, 1]\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "# í…ì„œë¡œ ë³€í™˜í•˜ì—¬ GPUë¡œ ì´ë™\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"--- í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ì ìš© ---\")\n",
    "print(f\"ê³„ì‚°ëœ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (Class 0, Class 1 ìˆœì„œ): {class_weights_tensor}\")\n",
    "print(\"Class 1(toxic)ì— ë” ë†’ì€ í˜ë„í‹°ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.\")\n",
    "# --- [ìƒˆ ë‹¨ê³„ ì¢…ë£Œ] ---\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds_logits = p.predictions # (batch_size, 2) í˜•íƒœì˜ ë¡œì§“\n",
    "    labels = p.label_ids # (batch_size,) í˜•íƒœì˜ 0 ë˜ëŠ” 1\n",
    "\n",
    "    # 1. ë¡œì§“ -> 0/1 ì˜ˆì¸¡ (F1, Accuracyìš©)\n",
    "    # ë¡œì§“ ì¤‘ ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤(0 ë˜ëŠ” 1)ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©\n",
    "    preds_binary = np.argmax(preds_logits, axis=1)\n",
    "\n",
    "    # 2. ë¡œì§“ -> í™•ë¥  (AUCìš©)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    # í´ë˜ìŠ¤ 1('toxic')ì— ëŒ€í•œ í™•ë¥ \n",
    "    probs_class_1 = softmax(torch.tensor(preds_logits)).numpy()[:, 1]\n",
    "\n",
    "    # 3. ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, probs_class_1)\n",
    "    except ValueError:\n",
    "        auc = 0.5\n",
    "    \n",
    "    # F1 ìŠ¤ì½”ì–´ ê³„ì‚° (zero_division=0ì€ ê²½ê³  ëŒ€ì‹  0.0 ë°˜í™˜)\n",
    "    # 'macro' F1: í´ë˜ìŠ¤ë³„ F1ì„ ê³„ì‚° í›„ ë‹¨ìˆœ í‰ê·  (ë¶ˆê· í˜• ë°ì´í„° í‰ê°€ì— ì¤‘ìš”)\n",
    "    f1_macro = f1_score(labels, preds_binary, average='macro', zero_division=0)\n",
    "    # 'weighted' F1: í´ë˜ìŠ¤ë³„ F1ì„ ê³„ì‚° í›„ ìƒ˜í”Œ ìˆ˜ë¡œ ê°€ì¤‘ í‰ê· \n",
    "    f1_weighted = f1_score(labels, preds_binary, average='weighted', zero_division=0)\n",
    "    # F1 (class 1, toxic): ì†Œìˆ˜ í´ë˜ìŠ¤ì¸ 'toxic'ë§Œì˜ F1 (ê°€ì¥ ì¤‘ìš”í•  ìˆ˜ ìˆìŒ)\n",
    "    f1_toxic_class = f1_score(labels, preds_binary, pos_label=1, average='binary', zero_division=0)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds_binary)\n",
    "\n",
    "    # ë¡œê·¸ì— ë‚¨ê¸¸ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "    return {\n",
    "        \"roc_auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,           # <--- ì´ ì§€í‘œë¥¼ ëª¨ë‹ˆí„°ë§í•  ì˜ˆì •\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"f1_toxic_class\": f1_toxic_class # <--- ì†Œìˆ˜ í´ë˜ìŠ¤ F1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01110b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì»¤ìŠ¤í…€ WeightedTrainerë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
      "í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269962/142729437.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3144' max='3144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3144/3144 43:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Toxic Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.374421</td>\n",
       "      <td>0.952656</td>\n",
       "      <td>0.910937</td>\n",
       "      <td>0.800756</td>\n",
       "      <td>0.920634</td>\n",
       "      <td>0.652591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.269798</td>\n",
       "      <td>0.967311</td>\n",
       "      <td>0.884634</td>\n",
       "      <td>0.770295</td>\n",
       "      <td>0.901417</td>\n",
       "      <td>0.608233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.266201</td>\n",
       "      <td>0.964513</td>\n",
       "      <td>0.942653</td>\n",
       "      <td>0.847539</td>\n",
       "      <td>0.944969</td>\n",
       "      <td>0.727118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.266400</td>\n",
       "      <td>0.265615</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.920734</td>\n",
       "      <td>0.819021</td>\n",
       "      <td>0.928793</td>\n",
       "      <td>0.683345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.215029</td>\n",
       "      <td>0.972543</td>\n",
       "      <td>0.919705</td>\n",
       "      <td>0.818256</td>\n",
       "      <td>0.928117</td>\n",
       "      <td>0.682469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.321059</td>\n",
       "      <td>0.956380</td>\n",
       "      <td>0.885753</td>\n",
       "      <td>0.773743</td>\n",
       "      <td>0.902544</td>\n",
       "      <td>0.614549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.268867</td>\n",
       "      <td>0.972827</td>\n",
       "      <td>0.929814</td>\n",
       "      <td>0.833137</td>\n",
       "      <td>0.935899</td>\n",
       "      <td>0.706125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.210802</td>\n",
       "      <td>0.976593</td>\n",
       "      <td>0.911832</td>\n",
       "      <td>0.808862</td>\n",
       "      <td>0.922368</td>\n",
       "      <td>0.668572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.227803</td>\n",
       "      <td>0.975357</td>\n",
       "      <td>0.939879</td>\n",
       "      <td>0.849466</td>\n",
       "      <td>0.943856</td>\n",
       "      <td>0.732803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.205296</td>\n",
       "      <td>0.977611</td>\n",
       "      <td>0.892865</td>\n",
       "      <td>0.784126</td>\n",
       "      <td>0.908087</td>\n",
       "      <td>0.630914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.366074</td>\n",
       "      <td>0.973950</td>\n",
       "      <td>0.953746</td>\n",
       "      <td>0.866474</td>\n",
       "      <td>0.953814</td>\n",
       "      <td>0.758524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.200116</td>\n",
       "      <td>0.978483</td>\n",
       "      <td>0.903109</td>\n",
       "      <td>0.798074</td>\n",
       "      <td>0.915904</td>\n",
       "      <td>0.652439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.979364</td>\n",
       "      <td>0.885887</td>\n",
       "      <td>0.775656</td>\n",
       "      <td>0.902889</td>\n",
       "      <td>0.618399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.207764</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.919079</td>\n",
       "      <td>0.820727</td>\n",
       "      <td>0.928161</td>\n",
       "      <td>0.687942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.220025</td>\n",
       "      <td>0.977831</td>\n",
       "      <td>0.932588</td>\n",
       "      <td>0.840913</td>\n",
       "      <td>0.938622</td>\n",
       "      <td>0.720149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.221571</td>\n",
       "      <td>0.979080</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.853206</td>\n",
       "      <td>0.944820</td>\n",
       "      <td>0.739973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.217649</td>\n",
       "      <td>0.978336</td>\n",
       "      <td>0.941266</td>\n",
       "      <td>0.855121</td>\n",
       "      <td>0.945509</td>\n",
       "      <td>0.743404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.230500</td>\n",
       "      <td>0.179835</td>\n",
       "      <td>0.979747</td>\n",
       "      <td>0.914113</td>\n",
       "      <td>0.814678</td>\n",
       "      <td>0.924509</td>\n",
       "      <td>0.678930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.243914</td>\n",
       "      <td>0.980339</td>\n",
       "      <td>0.946231</td>\n",
       "      <td>0.863567</td>\n",
       "      <td>0.949490</td>\n",
       "      <td>0.757368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.980316</td>\n",
       "      <td>0.910803</td>\n",
       "      <td>0.809789</td>\n",
       "      <td>0.921939</td>\n",
       "      <td>0.671174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.182566</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.926728</td>\n",
       "      <td>0.832911</td>\n",
       "      <td>0.934210</td>\n",
       "      <td>0.707709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>0.224929</td>\n",
       "      <td>0.978721</td>\n",
       "      <td>0.940729</td>\n",
       "      <td>0.853647</td>\n",
       "      <td>0.944986</td>\n",
       "      <td>0.740755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.220100</td>\n",
       "      <td>0.177915</td>\n",
       "      <td>0.981047</td>\n",
       "      <td>0.928115</td>\n",
       "      <td>0.835848</td>\n",
       "      <td>0.935420</td>\n",
       "      <td>0.712779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>0.191873</td>\n",
       "      <td>0.981289</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.854981</td>\n",
       "      <td>0.945110</td>\n",
       "      <td>0.743585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.186072</td>\n",
       "      <td>0.981582</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.832658</td>\n",
       "      <td>0.933735</td>\n",
       "      <td>0.707730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.190621</td>\n",
       "      <td>0.981978</td>\n",
       "      <td>0.937240</td>\n",
       "      <td>0.850354</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.736328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.190859</td>\n",
       "      <td>0.982504</td>\n",
       "      <td>0.941042</td>\n",
       "      <td>0.856733</td>\n",
       "      <td>0.945654</td>\n",
       "      <td>0.746831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.188954</td>\n",
       "      <td>0.982465</td>\n",
       "      <td>0.935093</td>\n",
       "      <td>0.847412</td>\n",
       "      <td>0.940997</td>\n",
       "      <td>0.731743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.185091</td>\n",
       "      <td>0.982457</td>\n",
       "      <td>0.930843</td>\n",
       "      <td>0.840587</td>\n",
       "      <td>0.937636</td>\n",
       "      <td>0.720636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.191793</td>\n",
       "      <td>0.982537</td>\n",
       "      <td>0.938806</td>\n",
       "      <td>0.853204</td>\n",
       "      <td>0.943900</td>\n",
       "      <td>0.741105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.184882</td>\n",
       "      <td>0.982515</td>\n",
       "      <td>0.936658</td>\n",
       "      <td>0.849627</td>\n",
       "      <td>0.942185</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì™„ë£Œ. ìµœì¢… í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [88/88 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²€ì¦ ê²°ê³¼ (Validation Set): {'eval_loss': 0.42328763008117676, 'eval_roc_auc': 0.9755133376934639, 'eval_accuracy': 0.9556251397897562, 'eval_f1_macro': 0.8700489816170328, 'eval_f1_weighted': 0.9553704067242513, 'eval_f1_toxic_class': 0.764594209776934, 'eval_runtime': 45.6361, 'eval_samples_per_second': 489.853, 'eval_steps_per_second': 1.928, 'epoch': 1.0}\n",
      "ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì´ './best_toxic_model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "# Trainerë¥¼ ìƒì†ë°›ì•„ compute_loss ë©”ì„œë“œë§Œ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    # **kwargsë¥¼ ì¶”ê°€í•˜ì—¬ ì˜ˆìƒì¹˜ ëª»í•œ ì¶”ê°€ ì¸ìë“¤ì„ ëª¨ë‘ ë°›ìŠµë‹ˆë‹¤.\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             \n",
    "    num_train_epochs=1,                 \n",
    "    per_device_train_batch_size=64,     \n",
    "    per_device_eval_batch_size=256,   \n",
    "    warmup_steps=100,                   \n",
    "    weight_decay=0.01,                  \n",
    "    logging_dir='./logs',               \n",
    "    logging_steps=100,\n",
    "    eval_steps=100,                  \n",
    "    eval_strategy=\"steps\",              \n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,        \n",
    "    metric_for_best_model=\"f1_macro\",   \n",
    "    greater_is_better=True,             \n",
    "    fp16=True,                          \n",
    ")\n",
    "\n",
    "print(\"ì»¤ìŠ¤í…€ WeightedTrainerë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\")\n",
    "trainer = WeightedTrainer(  \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    class_weights=class_weights_tensor  \n",
    ")\n",
    "\n",
    "print(\"í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"í•™ìŠµ ì™„ë£Œ. ìµœì¢… í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"ìµœì¢… ê²€ì¦ ê²°ê³¼ (Validation Set): {eval_results}\")\n",
    "\n",
    "# í•™ìŠµëœ ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model(\"./best_toxic_model\")\n",
    "tokenizer.save_pretrained(\"./best_toxic_model\")\n",
    "print(\"ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì´ './best_toxic_model'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73468c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from ./best_toxic_model...\n",
      "Tokenizing test data... (This may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e78101f0e8405faf2ea0a6e59ceaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ì´ì „ì— í•™ìŠµí•˜ê³  ì €ì¥í•œ ëª¨ë¸ì˜ ê²½ë¡œ\n",
    "MODEL_PATH = \"./best_toxic_model\" \n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ íŒŒì¼\n",
    "TEST_FILE = \"Data/test.csv\" \n",
    "\n",
    "# ì œì¶œí•  íŒŒì¼ ì´ë¦„\n",
    "SUBMISSION_FILE = \"submission22.csv\"\n",
    "\n",
    "TEXT_COLUMN = \"content\"\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# íŠœë‹ëœ í† í°ë‚˜ì´ì € ë¡œë“œ\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=2, \n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "model.to(device) \n",
    "model.eval() \n",
    "\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "\n",
    "# Pandas DataFrameì„ Hugging Face Dataset ê°ì²´ë¡œ ë³€í™˜\n",
    "test_dataset = HfDataset.from_pandas(test_df[[TEXT_COLUMN]])\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í† í°í™”\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        padding=\"max_length\", # ëª¨ë“  ì…ë ¥ì„ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©\n",
    "        truncation=True,      # ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” í…ìŠ¤íŠ¸ëŠ” ìë¦„\n",
    "        max_length=512        # í•™ìŠµ ì‹œ ì‚¬ìš©í–ˆë˜ max_lenê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing test data... (This may take a while)\")\n",
    "\n",
    "# .map()ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ë¥¼ ì¼ê´„ ì ìš©\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ëª¨ë¸ì´ ì…ë ¥ìœ¼ë¡œ ë°›ì§€ ì•ŠëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì€ ì œê±°\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([TEXT_COLUMN])\n",
    "\n",
    "# ì¶”ë¡ (Inference)ì„ ìœ„í•œ ìµœì†Œí•œì˜ TrainingArguments ì„¤ì •\n",
    "inference_args = TrainingArguments(\n",
    "    output_dir='./results_inference',       \n",
    "    per_device_eval_batch_size=256,    # ë°°ì¹˜ í¬ê¸° (\n",
    "    fp16=True if device == \"cuda\" else False, # GPU ì‚¬ìš© ì‹œ ì˜ˆì¸¡ ì†ë„ í–¥ìƒ (fp16)\n",
    "    do_train=False,\n",
    "    do_eval=False,\n",
    "    do_predict=True,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ê³¼ ì¶”ë¡  ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=inference_args\n",
    ")\n",
    "\n",
    "# .predict() ë©”ì„œë“œ í˜¸ì¶œí•˜ì—¬ ì¶”ë¡  ì‹¤í–‰\n",
    "print(\"Running predictions...\")\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# `predictions.predictions`ì— ëª¨ë¸ì˜ ì›ë³¸ ì¶œë ¥(logits)ì´ numpy ë°°ì—´ë¡œ ì €ì¥ë¨\n",
    "raw_logits = predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4e5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Softmax to get probabilities...\n",
      "Converting probabilities to binary (0/1) predictions with threshold 0.5...\n",
      "Creating submission file: submission22.csv\n",
      "Done. Submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Softmax to get probabilities...\")\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "probs_tensor = softmax(torch.tensor(raw_logits))\n",
    "probs_class_1 = probs_tensor.numpy()[:, 1] \n",
    "\n",
    "print(\"Converting probabilities to binary (0/1) predictions with threshold 0.5...\")\n",
    "binary_predictions = (probs_class_1 >= 0.5).astype(int) \n",
    "\n",
    "print(f\"Creating submission file: {SUBMISSION_FILE}\")\n",
    "\n",
    "submission_df = pd.DataFrame(binary_predictions, columns=[TARGET_COLUMN])\n",
    "submission_df['id'] = test_df['id']\n",
    "submission_df = submission_df[['id'] + [TARGET_COLUMN]]\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(\"Done. Submission file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027677a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading answer key from Data/test_labels.csv for evaluation...\n",
      "Merging predictions and answers...\n",
      "Total scored test samples: 63812\n",
      "\n",
      "--- 1. 'toxic' ì—´ ì •í™•ë„ (Accuracy) ---\n",
      "('toxic' ë ˆì´ë¸”ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨): 0.7982\n",
      "\n",
      "--- 2. 'toxic' ì—´ ìƒì„¸ ë¦¬í¬íŠ¸ (Precision, Recall, F1-Score) ---\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "toxic=0 (non-toxic)       0.80      0.99      0.88     49402\n",
      "    toxic=1 (toxic)       0.80      0.14      0.24     14410\n",
      "\n",
      "           accuracy                           0.80     63812\n",
      "          macro avg       0.80      0.57      0.56     63812\n",
      "       weighted avg       0.80      0.80      0.74     63812\n",
      "\n",
      "Done. Submission file created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "ANSWER_FILE = \"Data/test_labels.csv\"\n",
    "\n",
    "\n",
    "print(f\"\\nLoading answer key from {ANSWER_FILE} for evaluation...\")\n",
    "answer_df = pd.read_csv(ANSWER_FILE)\n",
    "\n",
    "# 1. ì˜ˆì¸¡(submission_df)ê³¼ ì •ë‹µ(answer_df)ì„ 'id' ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
    "#    submission_dfì˜ 'toxic' ì—´ê³¼ answer_dfì˜ 'toxic' ì—´ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "print(\"Merging predictions and answers...\")\n",
    "merged_df = pd.merge(\n",
    "        submission_df, \n",
    "        answer_df, \n",
    "        on=\"id\", \n",
    "        suffixes=('_pred', '_true') # 'toxic_pred'ì™€ 'toxic_true' ì—´ì´ ìƒì„±ë¨\n",
    ")\n",
    "\n",
    "scored_df = merged_df[merged_df['toxic_true'] != -1].copy()\n",
    "\n",
    "if scored_df.empty:\n",
    "    print(\"ì˜¤ë¥˜: ì •ë‹µì§€ì™€ ì˜ˆì¸¡ì„ ë³‘í•©í–ˆìœ¼ë‚˜ ìœ íš¨í•œ (non -1) í–‰ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"Total scored test samples: {len(scored_df)}\")\n",
    "\n",
    "        # 3. í‰ê°€ë¥¼ ìœ„í•´ 'toxic' ì—´ì˜ ì˜ˆì¸¡(y_pred)ê³¼ ì •ë‹µ(y_true) ë°°ì—´ ë¶„ë¦¬\n",
    "    y_pred = scored_df['toxic_pred'].values\n",
    "    y_true = scored_df['toxic_true'].values\n",
    "        \n",
    "        # 'toxic' ì—´ì— ëŒ€í•œ ì •í™•ë„ ì ìˆ˜ ê³„ì‚°\n",
    "        \n",
    "        # 'toxic' ì—´ ì •í™•ë„\n",
    "    toxic_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(\"\\n--- 1. 'toxic' ì—´ ì •í™•ë„ (Accuracy) ---\")\n",
    "    print(f\"('toxic' ë ˆì´ë¸”ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ë¹„ìœ¨): {toxic_accuracy:.4f}\")\n",
    "\n",
    "        # 'toxic' ì§€í‘œ (Precision, Recall, F1-Score)\n",
    "    print(\"\\n--- 2. 'toxic' ì—´ ìƒì„¸ ë¦¬í¬íŠ¸ (Precision, Recall, F1-Score) ---\")\n",
    "        \n",
    "    report = classification_report(\n",
    "            y_true, \n",
    "            y_pred,\n",
    "            target_names=['toxic=0 (non-toxic)', 'toxic=1 (toxic)'], \n",
    "            zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "print(\"Done. Submission file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc56132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    201194.000000\n",
       "mean        391.209315\n",
       "std         592.841466\n",
       "min           1.000000\n",
       "25%          93.000000\n",
       "50%         203.000000\n",
       "75%         431.000000\n",
       "max        5000.000000\n",
       "Name: comment_text, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text'].apply(len).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
